{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f395a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2 as cv\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, GlobalAveragePooling2D, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.models import Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "import seaborn as sns\n",
    "# import xgboost as xgb\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a54e2087",
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_data(folder_path, output_data):    #importing the data into the output_data list\n",
    "    for dirs in os.listdir(folder_path):\n",
    "        class_name = dirs\n",
    "        print(class_name)\n",
    "        new_path = os.path.join(folder_path, class_name)\n",
    "        for img in os.listdir(new_path):\n",
    "            img_arr = cv.imread(os.path.join(new_path, img), cv.IMREAD_GRAYSCALE)\n",
    "            resize = cv.resize(img_arr, (150,150))\n",
    "            output_data.append([resize, class_name])\n",
    "    return output_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97fd5290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aut\n",
      "Non\n",
      "aut\n",
      "Non\n"
     ]
    }
   ],
   "source": [
    "train_data = input_data(r'D:\\datasets\\AutismDataset\\train', [])\n",
    "test_data = input_data(r'D:\\datasets\\AutismDataset\\test', [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ceafc3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.shuffle(train_data)   #shuffling the data\n",
    "np.random.shuffle(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af9d638b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = []              #separating the image and labels from the train_data list\n",
    "train_labels = []\n",
    "for features, labels in train_data:\n",
    "    train_images.append(features)\n",
    "    train_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a15d6ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = []    #separating the image and labels from the test_data list\n",
    "test_labels = []\n",
    "for features, labels in test_data:\n",
    "    test_images.append(features)\n",
    "    test_labels.append(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43de4e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_enc = LabelEncoder()           # encoding the labels \n",
    "train_labels = label_enc.fit_transform(train_labels)\n",
    "test_labels = label_enc.transform(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "059b4d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.array(train_images)    #converting the images and labels into numpy array\n",
    "train_labels = np.array(train_labels)\n",
    "test_images = np.array(test_images)\n",
    "test_labels = np.array(test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8ea47a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = train_images/255    # normalizing the image pixels\n",
    "test_images = test_images/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6b1ce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = np.expand_dims(train_images, axis=3)      # adding a dimension on the images\n",
    "test_images = np.expand_dims(test_images, axis=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2332221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the train images (2540, 150, 150, 1)\n",
      "Shape of the train labels (2540,)\n",
      "Shape of the test images (300, 150, 150, 1)\n",
      "Shape of the test labels (300,)\n"
     ]
    }
   ],
   "source": [
    "print(f\"Shape of the train images {train_images.shape}\")\n",
    "print(f\"Shape of the train labels {train_labels.shape}\")\n",
    "print(f\"Shape of the test images {test_images.shape}\")\n",
    "print(f\"Shape of the test labels {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ec929717",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1 = Sequential()\n",
    "\n",
    "model1.add(Conv2D(32, (3, 3), input_shape=(150,150,1), activation=\"relu\"))\n",
    "model1.add(MaxPool2D(2,2))\n",
    "model1.add(Conv2D(64, (3, 3), activation=\"relu\"))\n",
    "model1.add(MaxPool2D(2,2))\n",
    "model1.add(Conv2D(128, (3, 3), activation=\"relu\"))\n",
    "model1.add(MaxPool2D(2,2))\n",
    "model1.add(Conv2D(256, (3, 3), activation=\"relu\"))\n",
    "model1.add(MaxPool2D(2,2))\n",
    "model1.add(Flatten())\n",
    "model1.add(Dense(256, activation=\"relu\"))\n",
    "model1.add(Dense(1, activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7fd613fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_4 (Conv2D)            (None, 148, 148, 32)      320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 15, 15, 256)       295168    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 7, 7, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 12544)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 256)               3211520   \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 3,599,617\n",
      "Trainable params: 3,599,617\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model1.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d9e616ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5a9e2d22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "80/80 [==============================] - 62s 769ms/step - loss: 0.5601 - accuracy: 0.7205 - val_loss: 0.5411 - val_accuracy: 0.7167\n",
      "Epoch 2/10\n",
      "80/80 [==============================] - 49s 618ms/step - loss: 0.5341 - accuracy: 0.7421 - val_loss: 0.5215 - val_accuracy: 0.7467\n",
      "Epoch 3/10\n",
      "80/80 [==============================] - 42s 527ms/step - loss: 0.5143 - accuracy: 0.7496 - val_loss: 0.4813 - val_accuracy: 0.7933\n",
      "Epoch 4/10\n",
      "80/80 [==============================] - 39s 490ms/step - loss: 0.4711 - accuracy: 0.7768 - val_loss: 0.5286 - val_accuracy: 0.7400\n",
      "Epoch 5/10\n",
      "80/80 [==============================] - 37s 457ms/step - loss: 0.4497 - accuracy: 0.7953 - val_loss: 0.5383 - val_accuracy: 0.7333\n",
      "Epoch 6/10\n",
      "80/80 [==============================] - 40s 499ms/step - loss: 0.4259 - accuracy: 0.8091 - val_loss: 0.4662 - val_accuracy: 0.7867\n",
      "Epoch 7/10\n",
      "80/80 [==============================] - 63s 790ms/step - loss: 0.3697 - accuracy: 0.8378 - val_loss: 0.4874 - val_accuracy: 0.7867\n",
      "Epoch 8/10\n",
      "80/80 [==============================] - 66s 820ms/step - loss: 0.3204 - accuracy: 0.8626 - val_loss: 0.5256 - val_accuracy: 0.7667\n",
      "Epoch 9/10\n",
      "80/80 [==============================] - 67s 834ms/step - loss: 0.2831 - accuracy: 0.8772 - val_loss: 0.5014 - val_accuracy: 0.7700\n",
      "Epoch 10/10\n",
      "80/80 [==============================] - 69s 858ms/step - loss: 0.2216 - accuracy: 0.9114 - val_loss: 0.5225 - val_accuracy: 0.8167\n"
     ]
    }
   ],
   "source": [
    "history1 = model1.fit(train_images, train_labels, validation_data=(test_images, test_labels), epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "25dd78d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "model1.save(\"Autism.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c558f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 normal\n",
      "2 normal\n",
      "3 aut\n",
      "4 normal\n",
      "5 normal\n",
      "6 aut\n",
      "7 normal\n",
      "8 normal\n",
      "9 normal\n",
      "10 aut\n",
      "11 normal\n",
      "12 normal\n",
      "13 normal\n",
      "14 aut\n",
      "15 normal\n",
      "16 normal\n",
      "17 normal\n",
      "18 normal\n",
      "19 normal\n",
      "20 normal\n",
      "21 normal\n",
      "22 normal\n",
      "23 normal\n",
      "24 normal\n",
      "25 normal\n",
      "26 normal\n",
      "27 normal\n",
      "28 normal\n",
      "29 normal\n",
      "30 normal\n",
      "31 normal\n",
      "32 normal\n",
      "33 normal\n",
      "34 normal\n",
      "35 normal\n",
      "36 normal\n",
      "37 normal\n",
      "38 normal\n",
      "39 normal\n",
      "40 normal\n",
      "41 normal\n",
      "42 normal\n",
      "43 normal\n",
      "44 normal\n",
      "45 normal\n",
      "46 normal\n",
      "47 normal\n",
      "48 normal\n",
      "49 normal\n",
      "50 normal\n",
      "51 normal\n",
      "52 normal\n",
      "53 normal\n",
      "54 normal\n",
      "55 normal\n",
      "56 normal\n",
      "57 aut\n",
      "58 aut\n",
      "59 aut\n",
      "60 normal\n",
      "61 normal\n",
      "62 normal\n",
      "63 normal\n",
      "64 normal\n",
      "65 normal\n",
      "66 aut\n",
      "67 normal\n",
      "68 normal\n",
      "69 normal\n",
      "70 aut\n",
      "71 normal\n",
      "72 normal\n",
      "73 aut\n",
      "74 normal\n",
      "75 aut\n",
      "76 aut\n",
      "77 normal\n",
      "78 normal\n",
      "79 normal\n",
      "80 aut\n",
      "81 normal\n",
      "82 normal\n",
      "83 normal\n",
      "84 normal\n"
     ]
    }
   ],
   "source": [
    "li=['Aut','Normal']\n",
    "import tensorflow\n",
    "import numpy as np\n",
    "from keras.preprocessing import image\n",
    "modelfile=\"Autism.hdf5\"\n",
    "model = tensorflow.keras.models.load_model(modelfile)\n",
    "from os import listdir\n",
    "test_dir='D:\\\\datasets\\\\AutismDataset\\\\test\\\\Non'\n",
    "files = listdir(test_dir)\n",
    "i=1\n",
    "for f in files:\n",
    "    filename=test_dir+\"\\\\\"+f\n",
    "    new_img = image.load_img(filename, target_size=(150, 150),color_mode=\"grayscale\")\n",
    "    img = image.img_to_array(new_img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = img / 255\n",
    "    pred=(model.predict(img) > 0.5).astype(\"int32\")\n",
    "    if pred[0][0]==1:\n",
    "        print(i,'aut')\n",
    "    else:\n",
    "        print(i,'normal')\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3853ef2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
